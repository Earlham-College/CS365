{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div id=\"header\"><p style=\"color:#3364ff; text-align:center; font-weight:bold; font-family:verdana; font-size:25px;\">AI and ML: k-Nearest Neighbor</p></div>\n",
    "\n",
    "[licenseBDG]: https://img.shields.io/badge/License-CC-orange?style=plastic\n",
    "[license]: https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en\n",
    "\n",
    "[mywebsiteBDG]:https://img.shields.io/badge/website-jaorduz.github.io-0abeeb?style=plastic\n",
    "[mywebsite]: https://jaorduz.github.io/\n",
    "\n",
    "[mygithubBDG-jaorduz]: https://img.shields.io/badge/jaorduz-repos-blue?logo=github&label=jaorduz&style=plastic\n",
    "[mygithub-jaorduz]: https://github.com/jaorduz/\n",
    "\n",
    "[mygithubBDG-jaorduc]: https://img.shields.io/badge/jaorduc-repos-blue?logo=github&label=jaorduc&style=plastic \n",
    "[mygithub-jaorduc]: https://github.com/jaorduc/\n",
    "\n",
    "[myXprofileBDG]: https://img.shields.io/static/v1?label=Follow&message=jaorduc&color=2ea44f&style=plastic&logo=X&logoColor=black\n",
    "[myXprofile]:https://twitter.com/jaorduc\n",
    "\n",
    "\n",
    "[![website - jaorduz.github.io][mywebsiteBDG]][mywebsite]\n",
    "[![Github][mygithubBDG-jaorduz]][mygithub-jaorduz]\n",
    "[![Github][mygithubBDG-jaorduc]][mygithub-jaorduc]\n",
    "[![Follow @jaorduc][myXprofileBDG]][myXprofile]\n",
    "[![CC License][licenseBDG]][license]\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p style=\"text-align:right; font-family:verdana;\"><a href=\"https://jaorduz.github.io/\" style=\"color:#3364ff; text-decoration:none;\";name = \"website\">@Javier Orduz</a></p>    \n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<h2>Objectives</h2>\n",
    "\n",
    "*   Develop a classification model using kNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab exercise, you will learn a popular machine kNN algorithm. You will use this classification algorithm to build a model from Breast Cancer Wisconsi. Then you will use the kNN to classify and support to the medical diagnostic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"https://#whatisdectre\">What is a kNN? Strengths and Limitations</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"https://#about_dataset\">About the dataset</a></li>\n",
    "            <li><a href=\"https://#entropy\">The K-Nearest Neighbors Algorithm: A Tool for Classification</a></li>   \n",
    "            <li><a href = \"https://#downloadingData\"> Downloading Dataset</a></li> \n",
    "        </ol>\n",
    "        <li><a href=\"https://#modeling\">Modeling</a></li>  \n",
    "        <ol>\n",
    "            <li><a href = \"https//#settingupData\">Setting up the Data </a></li>\n",
    "        </ol>      \n",
    "        <!-- <li><a href=\"https://#downloading_data\">Downloading the Data</a></li> -->\n",
    "        <li><a href=\"https://#pre-processing\">Pre-processing</a></li>\n",
    "        <ol>\n",
    "            <li><a href= \"https//#standardization\">Standardization of atributes and target</a></li>\n",
    "        </ol>\n",
    "        <li><a href=\"https://#classProcess\">Classification Process</a></li>        \n",
    "        <li><a href=\"https://#tranModel\">Training the model</a></li>\n",
    "            <ol>\n",
    "            <li><a href=\"optModel\">Model optimization and Metrics</a></li>\n",
    "            </ol>\n",
    "        <!-- <li><a href=\"https://#setting_up_tree\">Setting up Tree</a></li> -->\n",
    "        <!-- <li><a href=\"https://#prediction\">Prediction</a></li> -->\n",
    "        <li><a href=\"https://#evaluation\">Evaluation</a></li>\n",
    "        <!-- <li><a href=\"https://#visualization\">Visualization</a></li> -->\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "<header><h2>Importing libraries, packages and modules</h2></header>\n",
    "\n",
    "\n",
    "Import the Following Libraries:\n",
    "\n",
    "<ul>\n",
    "    <li> <b>numpy (as np)</b> </li>\n",
    "    <li> <b>pandas</b> </li>\n",
    "    <li> <b>DecisionTreeClassifier</b> from <b>sklearn.tree</b> </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<header><h2>1. What is kNN?: Strengths and Limitations</h2><header>\n",
    "\n",
    "\n",
    "provide me a definition for k nearest neighbor, give me references, and use equation is LateX typesetting.\n",
    "\n",
    "Key Strengths of KNN:\n",
    "1. Simple and Interpretable – Easy to implement and explain.  \n",
    "1. Adaptable – Works for both classification and regression.  \n",
    "1. No Training Phase – Learns dynamically from data.  \n",
    "\n",
    "Limitations:\n",
    "1. Computationally Expensive – Slower with large datasets.  \n",
    "1. Sensitive to Noise and irrelevant Features – Requires preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "<div id=\"about_dataset\">\n",
    "    <h2>1.1 About the dataset</h2>\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "1) ID number\n",
    "2) Diagnosis (M = malignant, B = benign)\n",
    "3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter)\n",
    "b) texture (standard deviation of gray-scale values)\n",
    "c) perimeter\n",
    "d) area\n",
    "e) smoothness (local variation in radius lengths)\n",
    "f) compactness (perimeter^2 / area - 1.0)\n",
    "g) concavity (severity of concave portions of the contour)\n",
    "h) concave points (number of concave portions of the contour)\n",
    "i) symmetry\n",
    "j) fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "The mean, standard error and \"worst\" or largest (mean of the three\n",
    "largest values) of these features were computed for each image,\n",
    "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
    "13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "All feature values are recoded with four significant digits.\n",
    "\n",
    "Missing attribute values: none\n",
    "\n",
    "Class distribution: 357 benign, 212 malignantsss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<header><h2>1.2 The K-Nearest Neighbors Algorithm: A Tool for Classification </h2><header>\n",
    "\n",
    "In the vast realm of machine learning algorithms, few techniques are as versatile and intuitive as the K-nearest neighbors (KNN) algorithm. As a nonparametric, instance-based learning method, KNN is widely used for classification, regression, and pattern recognition tasks. Its core principle relies on proximity-based decision-making, where new data points are classified based on the majority vote or average of their K closest neighbors in the feature space. Unlike many complex models, KNN requires no explicit training phase, making it exceptionally useful for real-time predictions. However, its performance depends heavily on distance metrics (Euclidean, Manhattan, or cosine similarity), the choice of K, and proper feature scaling. Studies have shown that KNN excels in low-dimensional datasets but may suffer from the \"curse of dimensionality\" in high-dimensional spaces (Cover & Hart, 1967; Altman, 1992).  \n",
    "\n",
    "\n",
    "<header><h3>1.2.1 Hyperparameter Tuning in k-Nearest Neighbors (k-NN)</h3><header>\n",
    "\n",
    "In k-Nearest Neighbors (k-NN), the primary hyperparameter to optimize is $ k $, the number of neighbors considered for prediction. The choice of $ k $ significantly impacts the model's bias-variance trade-off:  \n",
    "- A small $ k $ (e.g., $ k = 1 $) leads to low bias but high variance (overfitting).  \n",
    "- A large $ k $ (e.g., $ k = N $) increases bias but reduces variance (underfitting).  \n",
    "\n",
    "The optimal $ k $ can be selected via cross-validation:  \n",
    "$$\n",
    "k^* = \\argmin_{k} \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i - \\hat{y}_i^{(k)} \\right)^2  \n",
    "$$  \n",
    "where $ \\hat{y}_i^{(k)} $ is the prediction using $ k $ neighbors.  \n",
    "\n",
    " Additional Hyperparameters  \n",
    "1. Distance Metric ($ d $):  \n",
    "   - Euclidean distance (default for continuous features):  \n",
    "     $$\n",
    "     d(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{\\sum_{m=1}^{p} (x_{im} - x_{jm})^2}\n",
    "     $$  \n",
    "   - Manhattan distance (robust to outliers):  \n",
    "     $$\n",
    "     d(\\mathbf{x}_i, \\mathbf{x}_j) = \\sum_{m=1}^{p} |x_{im} - x_{jm}|\n",
    "     $$  \n",
    "   - Cosine similarity (for text/data with sparsity):  \n",
    "     $$\n",
    "     d(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{\\|\\mathbf{x}_i\\| \\|\\mathbf{x}_j\\|}\n",
    "     $$  \n",
    "\n",
    "2. Weighting Scheme:  \n",
    "   - Uniform weights: All neighbors contribute equally.  \n",
    "   - Distance-based weights: Closer neighbors have higher influence:  \n",
    "     $$\n",
    "     w_i = \\frac{1}{d(\\mathbf{x}_i, \\mathbf{x}_j)}\n",
    "     $$  \n",
    "\n",
    "3. Algorithm Selection:  \n",
    "   - Brute-force (exact search, slow for large $ N $).  \n",
    "   - KD-tree or Ball-tree (efficient for low/medium dimensions).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div id=\"downloading_data\"> \n",
    "    <h2>1.3 Downloading and exploring the Data</h2>\n",
    "    To download the data, we will use Ref. [[3](#references)]\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../DAT/data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div href=\"pre-processing\">\n",
    "    <h3>1.3.1 Dropping unnecessary columns</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['id','Unnamed: 32'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Remove the column containing the target name since it doesn't contain numeric values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div href=\"modeling\">\n",
    "    <h2>2. Modeling</h2>\n",
    "</div>\n",
    "\n",
    "**4.1** import  ```preprocessing``` from ```skelearn``` to convert to numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "<header><h3>2.1 Setting up the Data</h3> </header>\n",
    "\n",
    "For the following step we should check the ```scikit-learn``` documentation, and paricularly, focus on the ```sklearn.model_selection``` module in Scikit-Learn that provides functions for splitting data into training and test sets, evaluating machine learning models, and performing cross-validation. The ```train_test_split()``` function is the most commonly used function in the ```sklearn.model_selection module.```\n",
    "\n",
    "We will be using <b>train/test split</b> on our <b>decision tree</b>. Let's import <b>train_test_split</b> from <b>sklearn.cross_validation</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "```python \n",
    "from sklearn.model_selection import train_test_split\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now <b> train_test_split </b> will return 4 different parameters. We will name them:<br>\n",
    "X_trainset, X_testset, y_trainset, y_testset <br> <br>\n",
    "The <b> train_test_split </b> will need the parameters: <br>\n",
    "X, y, test_size=0.2, and random_state=2. <br> <br>\n",
    "The <b>X</b> and <b>y</b> are the arrays required before the split, the <b>test_size</b> represents the ratio of the testing dataset, and the <b>random_state</b> ensures that we obtain the same splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div href=\"pre-processing\">\n",
    "    <h2>3. Pre-processing</h2>\n",
    "\n",
    "```python \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will standard the data ```StandardScaler()``` to $\\mu = 0 $ and $\\sigma = 1$ in the z-score parameter. This is AKA z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<header><h3>3.1 Standardization of atributes and target</h3> </header>\n",
    "We apply standardization onto ```x``` for the training and testint data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(x_train)\n",
    "X_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div href=\"classProcess\">\n",
    "    <h2>4. Classification Process</h2>\n",
    "\n",
    "It requires the module\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div href=\"tranModel\">\n",
    "    <h2>5. Training the model</h2>\n",
    "</div>\n",
    "    For the following section we will need,\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires the module\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_neighbors\":[1,3,5],\n",
    "              \"weights\":[\"uniform\",\"distance\"]}\n",
    "model_optim = GridSearchCV(knn, parameters, cv=5,scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div href=\"optModel\">\n",
    "    <h3>5.1 Model optimization and Metrics</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,x,y) in zip([\"Train\",\"Test\"],[X_train,X_test],[y_train,y_test]):\n",
    "    print(\"Classification kNN\",i,\" report:\\n\",classification_report(y,model_optim.predict(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"most_frequent\",\"uniform\"]:\n",
    "    dummy = DummyClassifier(strategy=i).fit(X_train,y_train);\n",
    "    print(\"Classification \",i,\" test report:\",classification_report(y_test,dummy.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "<div id=\"evaluation\">\n",
    "    <h2>6. Evaluation</h2>\n",
    "    Next, let's import <b>metrics</b> from sklearn and check the accuracy of our model.\n",
    "</div>\n",
    "\n",
    "You will need\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over k values and compute training and test data accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here in the example shown above, we are creating a plot to see the k-value for which we have high accuracy.\n",
    "\n",
    "**Note:** This is a technique which is not used industry-wide to choose the correct value of n_neighbors. Instead, we do hyperparameter tuning to choose the value that gives the best performance [Ref. [5](#references)].\n",
    "<!-- **Accuracy classification score** computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in ```y_true.```\n",
    "\n",
    "In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<div id=\"versions\">\n",
    "    <h2>Packages, and module versions</h2>\n",
    "\n",
    "Let's check the version that can work with the different packages and modules.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, sklearn\n",
    "print(\"Python version\",sys.version)\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div id=\"exercises\">\n",
    "    <h2>Exercises</h2>\n",
    "\n",
    "1. There some websites to download database:\n",
    "    1. Research and describe all the steps, packages and coding parts of this NB,\n",
    "    1. Analyze the metrics, and\n",
    "    1. Create a report and include a short research about ```scikit-learn``` module and packages you used in this experiment [Ref. [12](#references)].\n",
    "\n",
    "1. Submmit your report inside the repository https://code.cs.earlham.edu/cs365s2025/ICAKaggle. \n",
    "    1. You can use this template https://www.overleaf.com/read/xqcnnnrsspcp\n",
    "1. Recommendations for the report: \n",
    "    1. Do not export as PDF the NB and submit it, instead you should create your own short report.\n",
    "    1. You can use part of the code or NB, and take care of your screeenshots.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div id=\"references\">\n",
    "    <h2>References</h2>\n",
    "\n",
    "[0] Cover, T., & Hart, P. (1967). *Nearest Neighbor Pattern Classification.* IEEE Transactions on Information Theory.  \n",
    "\n",
    "[1] Altman, N. S. (1992). *An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression.* The American Statistician.\n",
    "\n",
    "[2] kNN Introduction to ML Algorithms. https://medium.com/@sachinsoni600517/k-nearest-neighbours-introduction-to-machine-learning-algorithms-9dbc9d9fb3b2\n",
    "\n",
    "[3] Kaggle: kNN https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data\n",
    "\n",
    "[4] Kaggle: kNN https://www.kaggle.com/code/mmdatainfo/k-nearest-neighbors\n",
    "\n",
    "[5] geeksforgeeks: kNN https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/\n",
    "\n",
    "[6] Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning (2nd ed.). Springer.\n",
    "\n",
    "[7] Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective.  MIT Press.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
